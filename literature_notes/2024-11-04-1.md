# Sampling of LLM
* Nucleus sampling is the mainstream way. This samples the smallest subset of tokens such that their cumulative probability small or equals to p.
   $\sum_{x \in V^{(p)}} P(x|x_{1:i-1}) \geq p$ V^(p) is the smallest token set
  * This is better than beam-search; or argmax, because human language often not chooses the tokens with largest probability. Beam-search or argmax while achieving low perplexity than human,  the results are lack of diversity and repeating.
  * This is better than top-k. Because the k can be varied case-by-case. For high-entropy case, the k should be larger and high entropy cases smaller. Usually top-k is coupled with temperature [0-1] to have better results. E.g. low temperature to make the entropy lower. Its then trade-off the diversity.
## Reference
THE CURIOUS CASE OF NEURAL TEXT DeGENERATION
   