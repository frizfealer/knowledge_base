# Chain-of-Thought Verifier

* CoT let LLM generates intermediate steps to increase the generative performance. Here we can use Cot to make verifier more powerful.
* To train a CoT verifier, the paper trains the verifier on this dataset: $D_{CoT}=\{x, y_+, I_{CoT}, (v_{CoT}, I, \text{'Yes'})\} \cup\{x, y_-, I_{CoT}, (v_{CoT}, I, \text{'No'})\}$ Where $I_{CoT}$ =‘Let’s verify step by step'. $V_{CoT}$ re the verification rationales
* The paper further uses `reference-guided grading` to improve performance of CoT. Specifically, in training, they provides the training data with the correct solution in details.
	* E.g
	```
	You are a math teacher. Grade the Solution, verifying correctness step by step. Use Expected Answer to find any erroneous step in the Solution. At the end of the Solution verification, when you give your final grade, write it in the form "Verification: Is the answer correct (Yes/No)? X", where X is either Yes or No. 
	Question: {problem} 
	Solution: {solution} 
	Expected Answer: {a solution that arrives at the correct answer} <-> This part is `reference-guided grading`
	```

## Reference
Generative Verifiers: Reward Modeling as Next-Token Prediction
Judging llm-as-a-judge with mt-bench and chatbot arena
Self-consistency improves chain of thought reasoning in language models.
Training verifiers to solve math word problems